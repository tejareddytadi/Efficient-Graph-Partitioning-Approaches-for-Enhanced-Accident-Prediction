{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejareddytadi/Efficient-Graph-Partitioning-Approaches-for-Enhanced-Accident-Prediction/blob/main/Research_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44fa6d88-865e-4ae6-a607-ae2d561bdb77"
      },
      "source": [
        "\n",
        "TAP: A Comprehensive Data Repository for Traffic Accident Prediction in Transportation Networks. Baixiang Huang, Bryan Hooi, Kai Shu. [[link]](https://arxiv.org/pdf/2304.08640)"
      ],
      "id": "44fa6d88-865e-4ae6-a607-ae2d561bdb77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isNqzfEvTLd2"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install torch==1.13.0\n",
        "!pip install torch_geometric\n",
        "!pip install xgboost==0.90\n",
        "clear_output()"
      ],
      "id": "isNqzfEvTLd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtkainPKeGnw",
        "outputId": "d99c7773-f591-4ae3-cd36-6a6b6cc2741d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost version: 0.90\n",
            "CUDA version: 11.7\n",
            "PyTorch version: 1.13.0+cu117\n",
            "PyG version: 2.4.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch_geometric\n",
        "import xgboost as xgb\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print('XGBoost version:', xgb.__version__)\n",
        "print('CUDA version:', torch.version.cuda)\n",
        "print('PyTorch version:', torch.__version__)\n",
        "print('PyG version:', torch_geometric.__version__)"
      ],
      "id": "NtkainPKeGnw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh2XC4YYMKTe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Zh2XC4YYMKTe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dae266b3-599f-4e4c-bcbb-85aa9ddf7219"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import shutil\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os.path as osp\n",
        "import networkx as nx\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch_geometric\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "from torch_geometric.io import read_npz\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.inits import reset, uniform, zeros\n",
        "from torch_geometric.typing import OptTensor, OptPairTensor, Adj, Size\n",
        "from torch_geometric.data import Data, DataLoader, InMemoryDataset, download_url\n",
        "\n",
        "from pylab import cm\n",
        "from matplotlib import colors\n",
        "from IPython.display import clear_output\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from typing import Union, Tuple, Callable, Optional\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, roc_auc_score\n",
        "\n",
        "np.random.seed(7)\n",
        "# torch.manual_seed(7)\n",
        "plt.style.use(\"ggplot\")"
      ],
      "id": "dae266b3-599f-4e4c-bcbb-85aa9ddf7219"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84207d7a-11c9-45fb-8852-5ee4495aabdb"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "device, torch.cuda.current_device()"
      ],
      "id": "84207d7a-11c9-45fb-8852-5ee4495aabdb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84246c34-414d-4d74-85a5-cfbeb3b11446"
      },
      "outputs": [],
      "source": [
        "def read_npz(path):\n",
        "    with np.load(path, allow_pickle=True) as f:\n",
        "        return parse_npz(f)\n",
        "\n",
        "\n",
        "def parse_npz(f):\n",
        "    crash_time = f['crash_time']\n",
        "    x = torch.from_numpy(f['x']).to(torch.float)\n",
        "    coords = torch.from_numpy(f['coordinates']).to(torch.float)\n",
        "    edge_attr = torch.from_numpy(f['edge_attr']).to(torch.float)\n",
        "    cnt_labels = torch.from_numpy(f['cnt_labels']).to(torch.long)\n",
        "    occur_labels = torch.from_numpy(f['occur_labels']).to(torch.long)\n",
        "    edge_attr_dir = torch.from_numpy(f['edge_attr_dir']).to(torch.float)\n",
        "    edge_attr_ang = torch.from_numpy(f['edge_attr_ang']).to(torch.float)\n",
        "    severity_labels = torch.from_numpy(f['severity_8labels']).to(torch.long)\n",
        "    edge_index = torch.from_numpy(f['edge_index']).to(torch.long).t().contiguous()\n",
        "\n",
        "    num_edges = edge_index.size(1)\n",
        "    print(f\"Number of edges read: {num_edges}\")\n",
        "\n",
        "    return Data(x=x, y=occur_labels, severity_labels=severity_labels, edge_index=edge_index,\n",
        "                edge_attr=edge_attr, edge_attr_dir=edge_attr_dir, edge_attr_ang=edge_attr_ang,\n",
        "                coords=coords, cnt_labels=cnt_labels, crash_time=crash_time)\n",
        "\n",
        "\n"
      ],
      "id": "84246c34-414d-4d74-85a5-cfbeb3b11446"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25yJS7R95KNA"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = read_npz('/content/drive/MyDrive/miami_fl.npz')\n",
        "\n",
        "def create_networkx_graph(data):\n",
        "    # Create a MultiGraph (renamed to 'g')\n",
        "    g = nx.MultiGraph()\n",
        "\n",
        "    # Add nodes with their attributes\n",
        "    for i, label in enumerate(data.y):\n",
        "        g.add_node(i, label=label.item())\n",
        "\n",
        "    # Add all edges from the edge_index\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "\n",
        "    for edge in edge_index.T:\n",
        "        source, target = edge[:2]  # Take only the first two values\n",
        "        g.add_node(source, label=data.y[source].item())\n",
        "        g.add_node(target, label=data.y[target].item())\n",
        "        g.add_edge(source, target)\n",
        "\n",
        "    return g\n",
        "\n",
        "# Create MultiGraph from parsed data\n",
        "g = create_networkx_graph(data)\n",
        "\n",
        "# Print nodes and edges\n",
        "print(f\"Number of nodes in the graph: {g.number_of_nodes()}\")\n",
        "print(f\"Number of edges read: {len(data.edge_index.T)}\")\n",
        "print(f\"Number of edges in the graph: {g.number_of_edges()}\")\n",
        "\n",
        "# Visualize the graph\n",
        "pos = {i: (data.coords[i][0].item(), data.coords[i][1].item()) for i in range(len(data.coords))}\n",
        "labels = {i: data.y[i].item() for i in range(len(data.y))}\n",
        "\n",
        "nx.draw(g, pos, with_labels=False, labels=labels, node_size=0.5, node_color='lightblue', width=0.5, alpha=0.7, edge_color='black', font_size=10, font_color='black')\n",
        "plt.title('Miami')\n",
        "plt.show()\n"
      ],
      "id": "25yJS7R95KNA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbHwZulu3Hl8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['METIS_DLL'] = '/usr/lib/x86_64-linux-gnu/libmetis.so'\n",
        "!apt-get install -y libmetis-dev\n",
        "!pip install metis\n",
        "import metis"
      ],
      "id": "AbHwZulu3Hl8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJOaWvNNESS"
      },
      "source": [
        "normal metis"
      ],
      "id": "wcJOaWvNNESS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkhT2R243bfq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "np.random.seed(42)\n",
        "num_partitions = 10 # Change this to the desired number of partitions\n",
        "start_time = time.time()\n",
        "edgecuts, parts = metis.part_graph(g, num_partitions)\n",
        "partitioning_time = time.time() - start_time\n",
        "total_partitioned_nodes = 0\n",
        "# Print the number of nodes in each partition\n",
        "for partition_id in range(num_partitions):\n",
        "    partition_nodes = [node for node, part in enumerate(parts) if part == partition_id]\n",
        "    print(f\"Partition {partition_id}: {len(partition_nodes)} nodes\")\n",
        "    total_partitioned_nodes = total_partitioned_nodes + len(partition_nodes)\n",
        "print(f\"Total partitioned nodes: {total_partitioned_nodes} out of {len(g.nodes)} total nodes\")\n",
        "\n",
        "# Create subgraphs for each partition and visualize them\n",
        "subgraphs = []\n",
        "for partition_id in range(num_partitions):\n",
        "    partition_nodes = [node for node, part in enumerate(parts) if part == partition_id]\n",
        "    subgraph = g.subgraph(partition_nodes)\n",
        "    subgraphs.append(subgraph)\n",
        "\n",
        "# Visualize the partitioned subgraphs\n",
        "for i, subgraph in enumerate(subgraphs):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    pos = nx.spring_layout(subgraph)  # You can adjust the layout algorithm as needed\n",
        "    nx.draw(subgraph, pos, node_size=0.5)\n",
        "    plt.title(f'k=10')\n",
        "    plt.show()\n",
        "\n",
        "edges = []\n",
        "num_nodes = []\n",
        "for subgraph in subgraphs:\n",
        "    edges.append(len(subgraph.edges()))\n",
        "    num_nodes.append(len(subgraph.nodes()))\n",
        "\n",
        "edges = np.array(edges)\n",
        "nodes = np.array(num_nodes)\n",
        "g_nodes = len(g.nodes)\n",
        "g_edges = len(g.edges)\n",
        "\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"Sum of nodes : {sum_nodes}\")\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"Node loss : {(g_nodes - sum_nodes) / g_nodes}\")\n",
        "print(f\"Edge loss : {(g_edges - sum_edges) / g_edges}\")\n",
        "\n",
        "max_edges = np.max(edges)\n",
        "avg_edges = np.average(edges)\n",
        "balance = max_edges / avg_edges\n",
        "print(f\"Edge Balance : {balance}\")\n",
        "max_n = np.max(nodes)\n",
        "avg_n = np.average(nodes)\n",
        "Bal_n = max_n/avg_n\n",
        "print(f\"Node Balance :{Bal_n}\")\n",
        "print(f\"average nodes in partition :{avg_n}\")\n",
        "print(f\"average edges in partition :{avg_edges}\")\n",
        "print(f\"Partitioning time: {partitioning_time} seconds\")\n",
        "print(f\"Cut Size: {edgecuts}\")"
      ],
      "id": "CkhT2R243bfq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7J1kNrh1VgVZ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "np.random.seed(42)\n",
        "num_partitions = 30 # Change this to the desired number of partitions\n",
        "start_time = time.time()\n",
        "edgecuts, parts = metis.part_graph(g, num_partitions)\n",
        "partitioning_time = time.time() - start_time\n",
        "total_partitioned_nodes = 0\n",
        "# Print the number of nodes in each partition\n",
        "for partition_id in range(num_partitions):\n",
        "    partition_nodes = [node for node, part in enumerate(parts) if part == partition_id]\n",
        "    print(f\"Partition {partition_id}: {len(partition_nodes)} nodes\")\n",
        "    total_partitioned_nodes = total_partitioned_nodes + len(partition_nodes)\n",
        "print(f\"Total partitioned nodes: {total_partitioned_nodes} out of {len(g.nodes)} total nodes\")\n",
        "\n",
        "# Create subgraphs for each partition and visualize them\n",
        "subgraphs = []\n",
        "for partition_id in range(num_partitions):\n",
        "    partition_nodes = [node for node, part in enumerate(parts) if part == partition_id]\n",
        "    subgraph = g.subgraph(partition_nodes)\n",
        "    subgraphs.append(subgraph)\n",
        "\n",
        "# Visualize the partitioned subgraphs\n",
        "for i, subgraph in enumerate(subgraphs):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    pos = nx.spring_layout(subgraph)  # You can adjust the layout algorithm as needed\n",
        "    nx.draw(subgraph, pos, node_size=0.5)\n",
        "    plt.title(f'Partition {i} Subgraph')\n",
        "    plt.show()\n",
        "\n",
        "edges = []\n",
        "num_nodes = []\n",
        "for subgraph in subgraphs:\n",
        "    edges.append(len(subgraph.edges()))\n",
        "    num_nodes.append(len(subgraph.nodes()))\n",
        "\n",
        "edges = np.array(edges)\n",
        "nodes = np.array(num_nodes)\n",
        "g_nodes = len(g.nodes)\n",
        "g_edges = len(g.edges)\n",
        "\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"Sum of nodes : {sum_nodes}\")\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"Node loss : {(g_nodes - sum_nodes) / g_nodes}\")\n",
        "print(f\"Edge loss : {(g_edges - sum_edges) / g_edges}\")\n",
        "\n",
        "max_edges = np.max(edges)\n",
        "avg_edges = np.average(edges)\n",
        "balance = max_edges / avg_edges\n",
        "print(f\"Edge Balance : {balance}\")\n",
        "max_n = np.max(nodes)\n",
        "avg_n = np.average(nodes)\n",
        "Bal_n = max_n/avg_n\n",
        "print(f\"Node Balance :{Bal_n}\")\n",
        "print(f\"average nodes in partition :{avg_n}\")\n",
        "print(f\"average edges in partition :{avg_edges}\")\n",
        "print(f\"Partitioning time: {partitioning_time} seconds\")\n",
        "print(f\"Cut Size: {edgecuts}\")"
      ],
      "id": "7J1kNrh1VgVZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6hTD6GwWJg_"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "np.random.seed(42)\n",
        "num_partitions = 50 # Change this to the desired number of partitions\n",
        "start_time = time.time()\n",
        "edgecuts, parts = metis.part_graph(g, num_partitions)\n",
        "partitioning_time = time.time() - start_time\n",
        "total_partitioned_nodes = 0\n",
        "# Print the number of nodes in each partition\n",
        "for partition_id in range(num_partitions):\n",
        "    partition_nodes = [node for node, part in enumerate(parts) if part == partition_id]\n",
        "    print(f\"Partition {partition_id}: {len(partition_nodes)} nodes\")\n",
        "    total_partitioned_nodes = total_partitioned_nodes + len(partition_nodes)\n",
        "print(f\"Total partitioned nodes: {total_partitioned_nodes} out of {len(g.nodes)} total nodes\")\n",
        "\n",
        "# Create subgraphs for each partition and visualize them\n",
        "subgraphs = []\n",
        "for partition_id in range(num_partitions):\n",
        "    partition_nodes = [node for node, part in enumerate(parts) if part == partition_id]\n",
        "    subgraph = g.subgraph(partition_nodes)\n",
        "    subgraphs.append(subgraph)\n",
        "\n",
        "# Visualize the partitioned subgraphs\n",
        "for i, subgraph in enumerate(subgraphs):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    pos = nx.spring_layout(subgraph)  # You can adjust the layout algorithm as needed\n",
        "    nx.draw(subgraph, pos, node_size=0.5)\n",
        "    plt.title(f'Partition {i} Subgraph')\n",
        "    plt.show()\n",
        "\n",
        "edges = []\n",
        "num_nodes = []\n",
        "for subgraph in subgraphs:\n",
        "    edges.append(len(subgraph.edges()))\n",
        "    num_nodes.append(len(subgraph.nodes()))\n",
        "\n",
        "edges = np.array(edges)\n",
        "nodes = np.array(num_nodes)\n",
        "g_nodes = len(g.nodes)\n",
        "g_edges = len(g.edges)\n",
        "\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"Sum of nodes : {sum_nodes}\")\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"Node loss : {(g_nodes - sum_nodes) / g_nodes}\")\n",
        "print(f\"Edge loss : {(g_edges - sum_edges) / g_edges}\")\n",
        "\n",
        "max_edges = np.max(edges)\n",
        "avg_edges = np.average(edges)\n",
        "balance = max_edges / avg_edges\n",
        "print(f\"Edge Balance : {balance}\")\n",
        "max_n = np.max(nodes)\n",
        "avg_n = np.average(nodes)\n",
        "Bal_n = max_n/avg_n\n",
        "print(f\"Node Balance :{Bal_n}\")\n",
        "print(f\"average nodes in partition :{avg_n}\")\n",
        "print(f\"average edges in partition :{avg_edges}\")\n",
        "print(f\"Partitioning time: {partitioning_time} seconds\")\n",
        "print(f\"Cut Size: {edgecuts}\")"
      ],
      "id": "m6hTD6GwWJg_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IxSPiKJEEfXL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "np.random.seed(42)\n",
        "num_partitions = 100 # Change this to the desired number of partitions\n",
        "start_time = time.time()\n",
        "edgecuts, parts = metis.part_graph(g, num_partitions)\n",
        "partitioning_time = time.time() - start_time\n",
        "total_partitioned_nodes = 0\n",
        "# Print the number of nodes in each partition\n",
        "for partition_id in range(num_partitions):\n",
        "    partition_nodes = [node for node, part in enumerate(parts) if part == partition_id]\n",
        "    print(f\"Partition {partition_id}: {len(partition_nodes)} nodes\")\n",
        "    total_partitioned_nodes = total_partitioned_nodes + len(partition_nodes)\n",
        "print(f\"Total partitioned nodes: {total_partitioned_nodes} out of {len(g.nodes)} total nodes\")\n",
        "\n",
        "# Create subgraphs for each partition and visualize them\n",
        "subgraphs = []\n",
        "for partition_id in range(num_partitions):\n",
        "    partition_nodes = [node for node, part in enumerate(parts) if part == partition_id]\n",
        "    subgraph = g.subgraph(partition_nodes)\n",
        "    subgraphs.append(subgraph)\n",
        "\n",
        "# Visualize the partitioned subgraphs\n",
        "for i, subgraph in enumerate(subgraphs):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    pos = nx.spring_layout(subgraph)  # You can adjust the layout algorithm as needed\n",
        "    nx.draw(subgraph, pos, node_size=0.5)\n",
        "    plt.title(f'Partition {i} Subgraph')\n",
        "    plt.show()\n",
        "\n",
        "edges = []\n",
        "num_nodes = []\n",
        "for subgraph in subgraphs:\n",
        "    edges.append(len(subgraph.edges()))\n",
        "    num_nodes.append(len(subgraph.nodes()))\n",
        "\n",
        "edges = np.array(edges)\n",
        "nodes = np.array(num_nodes)\n",
        "g_nodes = len(g.nodes)\n",
        "g_edges = len(g.edges)\n",
        "\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"Sum of nodes : {sum_nodes}\")\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"Node loss : {(g_nodes - sum_nodes) / g_nodes}\")\n",
        "print(f\"Edge loss : {(g_edges - sum_edges) / g_edges}\")\n",
        "\n",
        "max_edges = np.max(edges)\n",
        "avg_edges = np.average(edges)\n",
        "balance = max_edges / avg_edges\n",
        "print(f\"Edge Balance : {balance}\")\n",
        "max_n = np.max(nodes)\n",
        "avg_n = np.average(nodes)\n",
        "Bal_n = max_n/avg_n\n",
        "print(f\"Node Balance :{Bal_n}\")\n",
        "print(f\"average nodes in partition :{avg_n}\")\n",
        "print(f\"average edges in partition :{avg_edges}\")\n",
        "print(f\"Partitioning time: {partitioning_time} seconds\")\n",
        "print(f\"Cut Size: {edgecuts}\")"
      ],
      "id": "IxSPiKJEEfXL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkSMsEXBryzn"
      },
      "source": [
        "for visualization purposes"
      ],
      "id": "TkSMsEXBryzn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StBS4QB7r0dG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "np.random.seed(42)\n",
        "num_partitions = 70 # Change this to the desired number of partitions\n",
        "start_time = time.time()\n",
        "edgecuts, parts = metis.part_graph(g, num_partitions)\n",
        "partitioning_time = time.time() - start_time\n",
        "total_partitioned_nodes = 0\n",
        "# Print the number of nodes in each partition\n",
        "for partition_id in range(num_partitions):\n",
        "    partition_nodes = [node for node, part in enumerate(parts) if part == partition_id]\n",
        "    print(f\"Partition {partition_id}: {len(partition_nodes)} nodes\")\n",
        "    total_partitioned_nodes = total_partitioned_nodes + len(partition_nodes)\n",
        "print(f\"Total partitioned nodes: {total_partitioned_nodes} out of {len(g.nodes)} total nodes\")\n",
        "\n",
        "# Create subgraphs for each partition and visualize them\n",
        "subgraphs = []\n",
        "for partition_id in range(num_partitions):\n",
        "    partition_nodes = [node for node, part in enumerate(parts) if part == partition_id]\n",
        "    subgraph = g.subgraph(partition_nodes)\n",
        "    subgraphs.append(subgraph)\n",
        "\n",
        "# Visualize the partitioned subgraphs\n",
        "\n",
        "\n",
        "edges = []\n",
        "num_nodes = []\n",
        "for subgraph in subgraphs:\n",
        "    edges.append(len(subgraph.edges()))\n",
        "    num_nodes.append(len(subgraph.nodes()))\n",
        "\n",
        "edges = np.array(edges)\n",
        "nodes = np.array(num_nodes)\n",
        "g_nodes = len(g.nodes)\n",
        "g_edges = len(g.edges)\n",
        "\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"Sum of nodes : {sum_nodes}\")\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"Node loss : {(g_nodes - sum_nodes) / g_nodes}\")\n",
        "print(f\"Edge loss : {(g_edges - sum_edges) / g_edges}\")\n",
        "\n",
        "max_edges = np.max(edges)\n",
        "avg_edges = np.average(edges)\n",
        "balance = max_edges / avg_edges\n",
        "print(f\"Edge Balance : {balance}\")\n",
        "max_n = np.max(nodes)\n",
        "avg_n = np.average(nodes)\n",
        "Bal_n = max_n/avg_n\n",
        "print(f\"Node Balance :{Bal_n}\")\n",
        "print(f\"average nodes in partition :{avg_n}\")\n",
        "print(f\"average edges in partition :{avg_edges}\")\n",
        "print(f\"Partitioning time: {partitioning_time} seconds\")\n",
        "print(f\"Cut Size: {edgecuts}\")"
      ],
      "id": "StBS4QB7r0dG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2AQU_7_bRax"
      },
      "source": [
        "dgl edge balanced metis parmetis"
      ],
      "id": "m2AQU_7_bRax"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1n13mjDbSn3"
      },
      "outputs": [],
      "source": [
        "!pip install dgl\n",
        "#!pip install torch==1.13.0\n",
        "import dgl"
      ],
      "id": "R1n13mjDbSn3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "for visvualizatioon purposes"
      ],
      "metadata": {
        "id": "lxQlwgQqtyEB"
      },
      "id": "lxQlwgQqtyEB"
    },
    {
      "cell_type": "code",
      "source": [
        "dgl.random.seed(42)\n",
        "D = dgl.from_networkx(g)\n",
        "D_sub = dgl.metis_partition(D,reshuffle = True,balance_edges=True,k = 70) #edge balanced\n",
        "sub_arr = []\n",
        "num_nodes = []\n",
        "num_edges = []\n",
        "array_of_elements = []\n",
        "for i in range( len(D_sub)):\n",
        "  subgraph = D_sub[i]\n",
        "  subgraph_nx = dgl.to_networkx(subgraph)\n",
        "  subgraph_nx = nx.to_undirected(subgraph_nx)\n",
        "  num_nodes.append(len(subgraph_nx.nodes))\n",
        "  num_edges.append(len(subgraph_nx.edges))\n",
        "  sub_arr.append(subgraph_nx)\n",
        "edges = np.array(num_edges)\n",
        "nodes = np.array(num_nodes)\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"sum of nodes : {sum_nodes}\")\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"sum of edges : {sum_edges}\")\n",
        "G_nodes = len(g.nodes)\n",
        "G_edges = len(g.edges)\n",
        "print(f\"node loss : {(G_nodes-sum_nodes)/G_nodes}\")\n",
        "print(f\"edge loss : {(G_edges-sum_edges)/G_edges}\")\n",
        "max = np.max(edges)\n",
        "avg = np.average(edges)\n",
        "Bal = max/avg\n",
        "print(f\"edge Balance : {Bal}\")\n",
        "max_n = np.max(nodes)\n",
        "avg_n = np.average(nodes)\n",
        "Bal_n = max_n/avg_n\n",
        "print(f\"node Balance : {Bal_n}\")\n",
        "print(f\"average nodes in partitions : {avg_n}\")\n",
        "print(f\"average edges in partition :{avg}\")\n"
      ],
      "metadata": {
        "id": "1Fub49Kmt0HT"
      },
      "id": "1Fub49Kmt0HT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfPgenomUmQF"
      },
      "outputs": [],
      "source": [
        "dgl.random.seed(42)\n",
        "D = dgl.from_networkx(g)\n",
        "D_sub = dgl.metis_partition(D,reshuffle = True,balance_edges=True,k = 10) #edge balanced\n",
        "sub_arr = []\n",
        "num_nodes = []\n",
        "num_edges = []\n",
        "array_of_elements = []\n",
        "for i in range( len(D_sub)):\n",
        "  subgraph = D_sub[i]\n",
        "  subgraph_nx = dgl.to_networkx(subgraph)\n",
        "  subgraph_nx = nx.to_undirected(subgraph_nx)\n",
        "  num_nodes.append(len(subgraph_nx.nodes))\n",
        "  num_edges.append(len(subgraph_nx.edges))\n",
        "  sub_arr.append(subgraph_nx)\n",
        "edges = np.array(num_edges)\n",
        "nodes = np.array(num_nodes)\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"sum of nodes : {sum_nodes}\")\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"sum of edges : {sum_edges}\")\n",
        "G_nodes = len(g.nodes)\n",
        "G_edges = len(g.edges)\n",
        "print(f\"node loss : {(G_nodes-sum_nodes)/G_nodes}\")\n",
        "print(f\"edge loss : {(G_edges-sum_edges)/G_edges}\")\n",
        "max = np.max(edges)\n",
        "avg = np.average(edges)\n",
        "Bal = max/avg\n",
        "print(f\"edge Balance : {Bal}\")\n",
        "max_n = np.max(nodes)\n",
        "avg_n = np.average(nodes)\n",
        "Bal_n = max_n/avg_n\n",
        "print(f\"node Balance : {Bal_n}\")\n",
        "print(f\"average nodes in partitions : {avg_n}\")\n",
        "print(f\"average edges in partition :{avg}\")\n",
        "for i,sub_eb in enumerate(sub_arr):\n",
        "  plt.figure()\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  pos = nx.spring_layout(sub_eb)\n",
        "  plt.title(f\"k=10\")\n",
        "  nx.draw(sub_eb,pos=pos,node_size = 0.5,with_labels = False,node_color = 'red',edge_color = 'black')\n",
        "  plt.show()"
      ],
      "id": "SfPgenomUmQF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-Yzle_WHf5r"
      },
      "outputs": [],
      "source": [
        "dgl.random.seed(42)\n",
        "D = dgl.from_networkx(g)\n",
        "D_sub = dgl.metis_partition(D,reshuffle = True,balance_edges=True,k = 30) #edge balanced\n",
        "sub_arr = []\n",
        "num_nodes = []\n",
        "num_edges = []\n",
        "array_of_elements = []\n",
        "for i in range( len(D_sub)):\n",
        "  subgraph = D_sub[i]\n",
        "  subgraph_nx = dgl.to_networkx(subgraph)\n",
        "  subgraph_nx = nx.to_undirected(subgraph_nx)\n",
        "  num_nodes.append(len(subgraph_nx.nodes))\n",
        "  num_edges.append(len(subgraph_nx.edges))\n",
        "  sub_arr.append(subgraph_nx)\n",
        "edges = np.array(num_edges)\n",
        "nodes = np.array(num_nodes)\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"sum of nodes : {sum_nodes}\")\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"sum of edges : {sum_edges}\")\n",
        "G_nodes = len(g.nodes)\n",
        "G_edges = len(g.edges)\n",
        "print(f\"node loss : {(G_nodes-sum_nodes)/G_nodes}\")\n",
        "print(f\"edge loss : {(G_edges-sum_edges)/G_edges}\")\n",
        "max = np.max(edges)\n",
        "avg = np.average(edges)\n",
        "Bal = max/avg\n",
        "print(f\"edge Balance : {Bal}\")\n",
        "max_n = np.max(nodes)\n",
        "avg_n = np.average(nodes)\n",
        "Bal_n = max_n/avg_n\n",
        "print(f\"node Balance : {Bal_n}\")\n",
        "print(f\"average nodes in partitions : {avg_n}\")\n",
        "print(f\"average edges in partition :{avg}\")\n",
        "for i,sub_eb in enumerate(sub_arr):\n",
        "  plt.figure()\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  pos = nx.spring_layout(sub_eb)\n",
        "  plt.title(f\"subgraph {i}\")\n",
        "  nx.draw(sub_eb,pos=pos,node_size = 0.5,with_labels = False,node_color = 'red',edge_color = 'black')\n",
        "  plt.show()"
      ],
      "id": "W-Yzle_WHf5r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkVsckHUIRF7"
      },
      "outputs": [],
      "source": [
        "dgl.random.seed(42)\n",
        "D = dgl.from_networkx(g)\n",
        "D_sub = dgl.metis_partition(D,reshuffle = True,balance_edges=True,k = 50) #edge balanced\n",
        "sub_arr = []\n",
        "num_nodes = []\n",
        "num_edges = []\n",
        "array_of_elements = []\n",
        "for i in range( len(D_sub)):\n",
        "  subgraph = D_sub[i]\n",
        "  subgraph_nx = dgl.to_networkx(subgraph)\n",
        "  subgraph_nx = nx.to_undirected(subgraph_nx)\n",
        "  num_nodes.append(len(subgraph_nx.nodes))\n",
        "  num_edges.append(len(subgraph_nx.edges))\n",
        "  sub_arr.append(subgraph_nx)\n",
        "edges = np.array(num_edges)\n",
        "nodes = np.array(num_nodes)\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"sum of nodes : {sum_nodes}\")\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"sum of edges : {sum_edges}\")\n",
        "G_nodes = len(g.nodes)\n",
        "G_edges = len(g.edges)\n",
        "print(f\"node loss : {(G_nodes-sum_nodes)/G_nodes}\")\n",
        "print(f\"edge loss : {(G_edges-sum_edges)/G_edges}\")\n",
        "max = np.max(edges)\n",
        "avg = np.average(edges)\n",
        "Bal = max/avg\n",
        "print(f\"edge Balance : {Bal}\")\n",
        "max_n = np.max(nodes)\n",
        "avg_n = np.average(nodes)\n",
        "Bal_n = max_n/avg_n\n",
        "print(f\"node Balance : {Bal_n}\")\n",
        "print(f\"average nodes in partitions : {avg_n}\")\n",
        "print(f\"average edges in partition :{avg}\")\n",
        "for i,sub_eb in enumerate(sub_arr):\n",
        "  plt.figure()\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  pos = nx.spring_layout(sub_eb)\n",
        "  plt.title(f\"subgraph {i}\")\n",
        "  nx.draw(sub_eb,pos=pos,node_size = 0.5,with_labels = False,node_color = 'red',edge_color = 'black')\n",
        "  plt.show()"
      ],
      "id": "WkVsckHUIRF7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9uxwc1LJgff"
      },
      "outputs": [],
      "source": [
        "dgl.random.seed(42)\n",
        "D = dgl.from_networkx(g)\n",
        "D_sub = dgl.metis_partition(D,reshuffle = True,balance_edges=True,k = 100) #edge balanced\n",
        "sub_arr = []\n",
        "num_nodes = []\n",
        "num_edges = []\n",
        "array_of_elements = []\n",
        "for i in range( len(D_sub)):\n",
        "  subgraph = D_sub[i]\n",
        "  subgraph_nx = dgl.to_networkx(subgraph)\n",
        "  subgraph_nx = nx.to_undirected(subgraph_nx)\n",
        "  num_nodes.append(len(subgraph_nx.nodes))\n",
        "  num_edges.append(len(subgraph_nx.edges))\n",
        "  sub_arr.append(subgraph_nx)\n",
        "edges = np.array(num_edges)\n",
        "nodes = np.array(num_nodes)\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"sum of nodes : {sum_nodes}\")\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"sum of edges : {sum_edges}\")\n",
        "G_nodes = len(g.nodes)\n",
        "G_edges = len(g.edges)\n",
        "print(f\"node loss : {(G_nodes-sum_nodes)/G_nodes}\")\n",
        "print(f\"edge loss : {(G_edges-sum_edges)/G_edges}\")\n",
        "max = np.max(edges)\n",
        "avg = np.average(edges)\n",
        "Bal = max/avg\n",
        "print(f\"edge Balance : {Bal}\")\n",
        "max_n = np.max(nodes)\n",
        "avg_n = np.average(nodes)\n",
        "Bal_n = max_n/avg_n\n",
        "print(f\"node Balance : {Bal_n}\")\n",
        "print(f\"average nodes in partitions : {avg_n}\")\n",
        "print(f\"average edges in partition :{avg}\")\n",
        "for i,sub_eb in enumerate(sub_arr):\n",
        "  plt.figure()\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  pos = nx.spring_layout(sub_eb)\n",
        "  plt.title(f\"subgraph {i}\")\n",
        "  nx.draw(sub_eb,pos=pos,node_size = 0.5,with_labels = False,node_color = 'red',edge_color = 'black')\n",
        "  plt.show()"
      ],
      "id": "H9uxwc1LJgff"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k1Pm36fhgex"
      },
      "source": [
        "random partitioning using dgl"
      ],
      "id": "2k1Pm36fhgex"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zFcsXsVLu-3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dgl.distributed\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'g' is your original NetworkX graph\n",
        "D = dgl.from_networkx(g)\n",
        "\n",
        "# Output path for saving partitioned subgraphs\n",
        "out_path = '/content/partitioned_graphs/'\n",
        "\n",
        "# Perform random graph partitioning with balanced edges\n",
        "D_sub = dgl.distributed.partition_graph(D, 'PARTITION', num_parts=15, part_method='random',\n",
        "                                       balance_edges=True, out_path=out_path)\n"
      ],
      "id": "9zFcsXsVLu-3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WGkQTAEXiCo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dgl.distributed\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'g' is your original NetworkX graph\n",
        "D = dgl.from_networkx(g)\n",
        "\n",
        "# Output path for saving partitioned subgraphs\n",
        "out_path = '/content/partitioned_graphs/'\n",
        "\n",
        "# Perform random graph partitioning with balanced edges\n",
        "D_sub = dgl.distributed.partition_graph(D, 'PARTITION', num_parts=15, part_method='random',\n",
        "                                       balance_edges=True, out_path=out_path)\n",
        "\n",
        "# Initialize lists for metrics\n",
        "edges = []\n",
        "num_nodes = []\n",
        "\n",
        "# Loop through the partitions\n",
        "for i in range(len(D_sub)):\n",
        "    subgraph = D_sub[i]\n",
        "    sub_nx = dgl.to_networkx(subgraph)\n",
        "\n",
        "    # Visualize the subgraph\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    pos = nx.spring_layout(sub_nx)\n",
        "    nx.draw(sub_nx, pos, node_size=0.5)\n",
        "    plt.title(f'Partition {i} Subgraph')\n",
        "    plt.show()\n",
        "\n",
        "    # Append the number of edges and nodes to the lists\n",
        "    edges.append(len(sub_nx.edges()))\n",
        "    num_nodes.append(len(sub_nx.nodes()))\n",
        "\n",
        "# Convert lists to arrays\n",
        "edges = np.array(edges)\n",
        "nodes = np.array(num_nodes)\n",
        "\n",
        "# Calculate sums and print results\n",
        "g_nodes = len(g.nodes())\n",
        "g_edges = len(g.edges())\n",
        "\n",
        "sum_nodes = np.sum(nodes)\n",
        "print(f\"Sum of nodes: {sum_nodes}\")\n",
        "print(f\"Node loss: {(g_nodes - sum_nodes) / g_nodes}\")\n",
        "\n",
        "sum_edges = np.sum(edges)\n",
        "print(f\"Sum of edges: {sum_edges}\")\n",
        "print(f\"Edge loss: {(g_edges - sum_edges) / g_edges}\")\n",
        "\n",
        "# Calculate max, avg, and balance\n",
        "max_edge = np.max(edges)\n",
        "avg_edge = np.average(edges)\n",
        "balance = max_edge / avg_edge\n",
        "print(f\"Balance: {balance}\")\n",
        "\n",
        "# Print edge cuts\n",
        "edgecuts = D_sub.ndata[dgl.distributed.NID]['_PARTITION']  # Access edge cuts from the node data\n",
        "print(f\"Edge cuts: {edgecuts}\")\n",
        "\n"
      ],
      "id": "1WGkQTAEXiCo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZlvGni6YemX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dgl.distributed\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'g' is your original NetworkX graph\n",
        "D = dgl.from_networkx(g)\n",
        "\n",
        "# Output path for saving partitioned subgraphs\n",
        "out_path = '/content/partitioned_graphs/'\n",
        "\n",
        "# Perform random graph partitioning with balanced edges\n",
        "D_sub = dgl.distributed.partition_graph(D, 'PARTITION', num_parts=15, part_method='random',\n",
        "                                       balance_edges=True, out_path=out_path)\n",
        "\n",
        "# Check if D_sub is None\n",
        "if D_sub is None:\n",
        "    print(\"D_sub is None\")\n",
        "else:\n",
        "    # Initialize lists for metrics\n",
        "    edges = []\n",
        "    num_nodes = []\n",
        "\n",
        "    # Loop through the partitions\n",
        "    for i in range(len(D_sub)):\n",
        "        subgraph = D_sub[i]\n",
        "        sub_nx = dgl.to_networkx(subgraph)\n",
        "\n",
        "        # Visualize the subgraph\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        pos = nx.spring_layout(sub_nx)\n",
        "        nx.draw(sub_nx, pos, node_size=0.5)\n",
        "        plt.title(f'Partition {i} Subgraph')\n",
        "        plt.show()\n",
        "\n",
        "        # Append the number of edges and nodes to the lists\n",
        "        edges.append(len(sub_nx.edges()))\n",
        "        num_nodes.append(len(sub_nx.nodes()))\n",
        "\n",
        "    # Convert lists to arrays\n",
        "    edges = np.array(edges)\n",
        "    nodes = np.array(num_nodes)\n",
        "\n",
        "    # Calculate sums and print results\n",
        "    g_nodes = len(g.nodes())\n",
        "    g_edges = len(g.edges())\n",
        "\n",
        "    sum_nodes = np.sum(nodes)\n",
        "    print(f\"Sum of nodes: {sum_nodes}\")\n",
        "    print(f\"Node loss: {(g_nodes - sum_nodes) / g_nodes}\")\n",
        "\n",
        "    sum_edges = np.sum(edges)\n",
        "    print(f\"Sum of edges: {sum_edges}\")\n",
        "    print(f\"Edge loss: {(g_edges - sum_edges) / g_edges}\")\n",
        "\n",
        "    max_edge = np.max(edges)\n",
        "    avg_edge = np.average(edges)\n",
        "    balance = max_edge / avg_edge\n",
        "    print(f\"Balance: {balance}\")\n",
        "    edgecuts = D_sub.ndata[dgl.distributed.NID]['_PARTITION']  # Access edge cuts from the node data\n",
        "    print(f\"Edge cuts: {edgecuts}\")"
      ],
      "id": "QZlvGni6YemX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr1ROPixhi_T"
      },
      "outputs": [],
      "source": [
        "D = dgl.from_networkx(g)\n",
        "out_path = '/content/partitioned_graphs/'\n",
        "\n",
        "# Perform random graph partitioning with balanced edges\n",
        "D_sub = dgl.distributed.partition_graph(D, 'PARTITION', num_parts=15, part_method='random',\n",
        "                                       balance_edges=True, out_path=out_path)\n"
      ],
      "id": "vr1ROPixhi_T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O07l7h-dtCaY"
      },
      "source": [
        "random parttioning"
      ],
      "id": "O07l7h-dtCaY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "ATXln7I7N06g",
        "outputId": "b1bb92f5-4c3a-417b-ed00-9bde6cd0597d"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c25c0fcef1f8>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Edge cut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0medge_cut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_partitioned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_partitioned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'partition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG_partitioned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Print metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-c25c0fcef1f8>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Edge cut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0medge_cut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_partitioned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_partitioned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'partition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG_partitioned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Print metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'partition'"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Your existing graph\n",
        "# or nx.DiGraph() if your graph is directed\n",
        "# Add nodes and edges to your existing graph g (not shown in the example)\n",
        "\n",
        "# Parameters for the random partition graph\n",
        "partition_sizes = [190, 500, 700, 800, 300, 1000, 900, 600, 800, 561]  # Adjust the sizes as needed\n",
        "p_in = 0.25  # Probability of edges within groups\n",
        "p_out = 0.01  # Probability of edges between groups\n",
        "\n",
        "# Create a new graph based on the random partition model\n",
        "G_partitioned = nx.random_partition_graph(partition_sizes, p_in, p_out)\n",
        "\n",
        "# Add 'partition' attribute to nodes in G_partitioned\n",
        "partition_info = G_partitioned.graph[\"partition\"]\n",
        "for i, size in enumerate(partition_info):\n",
        "    for node in G_partitioned.nodes():\n",
        "        if node < sum(partition_sizes[:i+1]) and node >= sum(partition_sizes[:i]):\n",
        "            G_partitioned.nodes[node]['partition'] = i\n",
        "\n",
        "# Now, you can add nodes and edges from your existing graph g to G_partitioned\n",
        "G_partitioned.add_nodes_from(g.nodes(data=True))\n",
        "G_partitioned.add_edges_from(g.edges())\n",
        "\n",
        "# Calculate metrics\n",
        "g_nodes = len(g.nodes())\n",
        "g_edges = len(g.edges())\n",
        "partitioned_nodes = sum(partition_sizes)\n",
        "\n",
        "# Node loss\n",
        "node_loss = (g_nodes - partitioned_nodes) / g_nodes\n",
        "\n",
        "# Edge loss\n",
        "edge_loss = (g_edges - G_partitioned.number_of_edges()) / g_edges\n",
        "\n",
        "# Calculate balance\n",
        "max_partition_size = max(partition_sizes)\n",
        "avg_partition_size = np.mean(partition_sizes)\n",
        "balance = max_partition_size / avg_partition_size\n",
        "\n",
        "# Edge cut\n",
        "edge_cut = sum(len(set(G_partitioned.neighbors(node)) - set(str(G_partitioned.nodes[node]['partition']))) for node in G_partitioned.nodes)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Node loss: {node_loss}\")\n",
        "print(f\"Edge loss: {edge_loss}\")\n",
        "print(f\"Balance: {balance}\")\n",
        "print(f\"Edge cut: {edge_cut}\")\n",
        "\"\"\"\n",
        "# Visualize the partitioned graph\n",
        "pos = nx.spring_layout(G_partitioned)\n",
        "nx.draw(G_partitioned, pos, with_labels=True, node_size=50, font_size=8)\n",
        "plt.show()\n",
        "\n",
        "# Visualize each subgraph separately\n",
        "for i, size in enumerate(partition_info):\n",
        "    subgraph_nodes = [node for node, data in G_partitioned.nodes(data=True) if data.get('partition') == i]\n",
        "    subgraph = G_partitioned.subgraph(subgraph_nodes)\n",
        "\n",
        "    # Plot subgraph\n",
        "    plt.figure()\n",
        "    pos_subgraph = nx.spring_layout(subgraph)\n",
        "    nx.draw(subgraph, pos_subgraph, with_labels=True, node_size=50, font_size=8)\n",
        "    plt.title(f\"Subgraph {i+1}\")\n",
        "    plt.show()\"\"\"\n"
      ],
      "id": "ATXln7I7N06g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRKdfPaC53ga"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Your existing graph\n",
        "# or nx.DiGraph() if your graph is directed\n",
        "# Add nodes and edges to your existing graph g (not shown in the example)\n",
        "\n",
        "# Parameters for the random partition graph\n",
        "partition_sizes = [190, 500, 700, 800, 300, 1000, 900, 600, 800, 561]  # Adjust the sizes as needed\n",
        "p_in = 0.25  # Probability of edges within groups\n",
        "p_out = 0.01  # Probability of edges between groups\n",
        "\n",
        "# Create a new graph based on the random partition model\n",
        "G_partitioned = nx.random_partition_graph(partition_sizes, p_in, p_out)\n",
        "\n",
        "# Add 'partition' attribute to nodes in G_partitioned\n",
        "partition_info = G_partitioned.graph[\"partition\"]\n",
        "for i, size in enumerate(partition_info):\n",
        "    for node in G_partitioned.nodes():\n",
        "        if node < sum(partition_sizes[:i+1]) and node >= sum(partition_sizes[:i]):\n",
        "            G_partitioned.nodes[node]['partition'] = i\n",
        "\n",
        "# Now, you can add nodes and edges from your existing graph g to G_partitioned\n",
        "G_partitioned.add_nodes_from(g.nodes(data=True))\n",
        "G_partitioned.add_edges_from(g.edges())\n",
        "\n",
        "# Print the number of nodes in each partition\n",
        "print(\"Number of partitions:\", len(partition_info))\n",
        "for i, size in enumerate(partition_info):\n",
        "    print(f\"Partition {i+1} size: {size} nodes\")\n",
        "\n",
        "# Visualize the partitioned graph\n",
        "pos = nx.spring_layout(G_partitioned)\n",
        "nx.draw(G_partitioned, pos, with_labels=True, node_size=50, font_size=8)\n",
        "plt.show()\n",
        "\n",
        "# Visualize each subgraph separately\n",
        "for i, size in enumerate(partition_info):\n",
        "    subgraph_nodes = [node for node, data in G_partitioned.nodes(data=True) if data.get('partition') == i]\n",
        "    subgraph = G_partitioned.subgraph(subgraph_nodes)\n",
        "\n",
        "    # Plot subgraph\n",
        "    plt.figure()\n",
        "    pos_subgraph = nx.spring_layout(subgraph)\n",
        "    nx.draw(subgraph, pos_subgraph, with_labels=True, node_size=50, font_size=8)\n",
        "    plt.title(f\"Subgraph {i+1}\")\n",
        "    plt.show()\n"
      ],
      "id": "nRKdfPaC53ga"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fdf4197-750c-4515-8a03-849fa4164a0c"
      },
      "outputs": [],
      "source": [
        "class TRAVELDataset(InMemoryDataset):\n",
        "    r\"\"\"The Traffic Accident Prediction (TAP) dataset introduced in the\n",
        "    `\"TAP: A Comprehensive Data Repository for Traffic Accident Prediction in Road Networks\"\n",
        "    <https://arxiv.org/pdf/2304.08640>`_ paper.\n",
        "    Nodes represent intersections and edges are roads.\n",
        "    Node and edge features represent embeddings of geospatial features.\n",
        "    The task is to predict the occurrence and severity of accidents on roadways.\n",
        "    For further information, please refer to the TAP repository.\n",
        "    `TAP\n",
        "    <https://github.com/baixianghuang/travel>`_\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory where the dataset should be saved.\n",
        "        name (string): The name of the dataset.\n",
        "        transform (callable, optional): A function/transform that takes in an\n",
        "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
        "            version. The data object will be transformed before every access.\n",
        "            (default: :obj:`None`)\n",
        "        pre_transform (callable, optional): A function/transform that takes in\n",
        "            an :obj:`torch_geometric.data.Data` object and returns a\n",
        "            transformed version. The data object will be transformed before\n",
        "            being saved to disk. (default: :obj:`None`)\n",
        "    \"\"\"\n",
        "    url = 'https://github.com/baixianghuang/travel/raw/main/TAP-city/{}.npz'\n",
        "    # url = 'https://github.com/baixianghuang/travel/raw/main/TAP-state/{}.npz'\n",
        "\n",
        "    def __init__(self, root: str, name: str,\n",
        "                 transform: Optional[Callable] = None,\n",
        "                 pre_transform: Optional[Callable] = None):\n",
        "        self.name = name.lower()\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self) -> str:\n",
        "        return osp.join(self.root, self.name, 'raw')\n",
        "\n",
        "    @property\n",
        "    def processed_dir(self) -> str:\n",
        "        return osp.join(self.root, self.name, 'processed')\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> str:\n",
        "        return f'{self.name}.npz'\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self) -> str:\n",
        "        return 'data.pt'\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.url.format(self.name), self.raw_dir)\n",
        "\n",
        "    def process(self):\n",
        "        data = read_npz(self.raw_paths[0])\n",
        "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
        "        data, slices = self.collate([data])\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f'{self.name.capitalize()}Full()'"
      ],
      "id": "0fdf4197-750c-4515-8a03-849fa4164a0c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbfd29ad-0c2c-4c1c-9d50-5e1e4190b9eb"
      },
      "outputs": [],
      "source": [
        "class TRAVELConv(MessagePassing):\n",
        "    r\"\"\"\n",
        "    Args:\n",
        "        in_channels (int or tuple): Size of each input sample, or :obj:`-1` to\n",
        "            derive the size from the first input(s) to the forward method.\n",
        "            A tuple corresponds to the sizes of source and target\n",
        "            dimensionalities.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        nn (torch.nn.Module): Multiple layers of non-linear transformations\n",
        "            that maps feature data of shape :obj:`[-1,\n",
        "            num_node_features + num_edge_features]` to shape\n",
        "            :obj:`[-1, new_dimension]`, *e.g.*, defined by\n",
        "            :class:`torch.nn.Sequential`.\n",
        "        aggr (string, optional): The aggregation scheme to use\n",
        "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
        "            (default: :obj:`\"add\"`)\n",
        "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
        "            not add the transformed root node features to the output.\n",
        "            (default: :obj:`True`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
        "                 out_channels: int, nn: Callable, aggr: str = 'add',\n",
        "                 root_weight: bool = True, bias: bool = True, **kwargs):\n",
        "        super(TRAVELConv, self).__init__(aggr=aggr, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.nn = nn\n",
        "        self.aggr = aggr\n",
        "\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "\n",
        "        self.in_channels_l = in_channels[0]\n",
        "\n",
        "        if root_weight:\n",
        "            self.root = Parameter(torch.Tensor(in_channels[1], out_channels))\n",
        "        else:\n",
        "            self.register_parameter('root', None)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        reset(self.nn)\n",
        "        if self.root is not None:\n",
        "            uniform(self.root.size(0), self.root)\n",
        "        zeros(self.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
        "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
        "        if isinstance(x, Tensor):\n",
        "            x: OptPairTensor = (x, x)\n",
        "\n",
        "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
        "\n",
        "        x_r = x[1]\n",
        "        if x_r is not None and self.root is not None:\n",
        "            out += torch.matmul(x_r, self.root)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            out += self.bias\n",
        "        return out\n",
        "\n",
        "\n",
        "    def message(self, x_i: Tensor, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
        "        inputs = torch.cat([x_j, edge_attr], dim=1)\n",
        "        return self.nn(inputs)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {}, aggr=\"{}\", nn={})'.format(self.__class__.__name__,\n",
        "                                                     self.in_channels,\n",
        "                                                     self.out_channels,\n",
        "                                                     self.aggr, self.nn)"
      ],
      "id": "cbfd29ad-0c2c-4c1c-9d50-5e1e4190b9eb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89d99aab-7311-41a1-842c-336b14064d43"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    logits, measures = model().detach(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        mea = f1_score(data.y[mask].cpu(), pred.cpu(), average='binary')\n",
        "        measures.append(mea)\n",
        "\n",
        "    label_pred = logits.max(1)[1]\n",
        "    mask = data.test_mask\n",
        "    scores = logits[mask][:,1]\n",
        "    pred = logits[mask].max(1)[1]\n",
        "    test_y = data.y[mask]\n",
        "\n",
        "    test_acc = pred.eq(test_y).sum().item() / mask.sum().item()\n",
        "    test_map = average_precision_score(test_y.cpu(), scores.cpu())\n",
        "    test_auc = roc_auc_score(test_y.cpu(), scores.cpu())\n",
        "    return measures, label_pred, test_acc, test_map, test_auc\n",
        "\n",
        "\n",
        "def train_loop(model, data, optimizer, num_epochs, model_name='', city_name=''):\n",
        "    epochs, train_measures, valid_measures, test_measures, test_accs, test_maps, test_aucs = [], [], [], [], [], [], []\n",
        "    for epoch in range(num_epochs):\n",
        "        train(model, data, optimizer)\n",
        "        log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
        "        measures, label_pred, test_acc, test_map, test_auc = test(model, data)\n",
        "        train_mea, valid_mea, test_mea = measures\n",
        "        epochs.append(epoch)\n",
        "        train_measures.append(train_mea)\n",
        "        valid_measures.append(valid_mea)\n",
        "        test_measures.append(test_mea)\n",
        "        test_aucs.append(test_auc)\n",
        "        test_accs.append(test_acc)\n",
        "        test_maps.append(test_map)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            clear_output(True)\n",
        "            fig, (ax1, ax) = plt.subplots(1, 2, figsize=(30, 12))\n",
        "            gdf_pred['label'] = label_pred.cpu().numpy()\n",
        "            for i in range(class_num):\n",
        "                G = nx.MultiGraph()\n",
        "                G.add_nodes_from(gdf_pred[gdf_pred['label'] == i].index)\n",
        "                sub1 = nx.draw(G, pos=pos_dict, ax=ax1, node_color=color_ls[i], node_size=10)\n",
        "\n",
        "            ax.text(1, 1, log.format(epoch, train_measures[-1], valid_measures[-1], test_measures[-1]), fontsize=18)\n",
        "            ax.plot(epochs, train_measures, \"r\", epochs, valid_measures, \"g\", epochs, test_measures, \"b\")\n",
        "            ax.set_ylim([0, 1])\n",
        "            ax.legend([\"train\", \"valid\", \"test\"])\n",
        "            ax1.legend([\"Negative\", \"Positive\"])\n",
        "            ax1.set_title(city_name+' '+model_name, y=-0.01)\n",
        "            plt.show()\n",
        "\n",
        "    select_idx = np.argmax(valid_measures[num_epochs//2:]) + num_epochs//2\n",
        "    final_test_mea = np.array(test_measures)[select_idx]\n",
        "    final_test_auc = np.array(test_aucs)[select_idx]\n",
        "    final_test_acc = np.array(test_accs)[select_idx]\n",
        "    final_test_map = np.array(test_maps)[select_idx]\n",
        "\n",
        "    print('Selected epoch {}'.format(select_idx))\n",
        "    print('F1 {:.5f} | AUC {:.5f} | Test Acc {:.5f} | MAP {:.5f}'.format(final_test_mea, final_test_auc, final_test_acc, final_test_map))\n",
        "    return (round(final_test_mea*100, 2), round(final_test_auc*100, 2), round(final_test_acc*100, 2), round(final_test_map*100, 2))"
      ],
      "id": "89d99aab-7311-41a1-842c-336b14064d43"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b695c80-fb16-4f1c-9616-ebfad04cea88"
      },
      "outputs": [],
      "source": [
        "# cities_sorted_by_accident.pkl is available in the directory `util` (https://github.com/baixianghuang/travel/tree/main/util)\n",
        "# with open('cities_sorted_by_accident.pkl', 'rb') as fp:\n",
        "#     all_city_ls = pickle.load(fp)\n",
        "\n",
        "# print('# cities:', len(all_city_ls))\n",
        "# for e in all_city_ls[:50]:\n",
        "#     print(e[0]+' ('+e[1]+')', end = ', ')\n",
        "# len(all_city_ls)"
      ],
      "id": "9b695c80-fb16-4f1c-9616-ebfad04cea88"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1351786-8616-44d2-aaf9-fb549327fee5"
      },
      "source": [
        "## Training"
      ],
      "id": "e1351786-8616-44d2-aaf9-fb549327fee5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70062849-08d5-4e12-aaf7-be33046f4bdd"
      },
      "outputs": [],
      "source": [
        "def draw_with_labels(df_nodes, model_name='test'):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    for i in range(class_num):\n",
        "        G = nx.MultiGraph()\n",
        "        G.add_nodes_from(df_nodes[df_nodes['label'] == i].index)\n",
        "        nx.draw(G, pos=pos_dict, node_color=color_ls[i], node_size=3, label=i)\n",
        "    plt.legend(labels=[\"Negative\", \"Positive\"], loc=\"upper right\", fontsize='small')\n",
        "    plt.title(model_name, y=-0.01)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "d=16\n",
        "p=0.5\n",
        "all_res = []\n",
        "color_ls = []\n",
        "class_num = 2\n",
        "num_epochs = 301\n",
        "file_path = 'exp/'\n",
        "cmap = cm.get_cmap('cool', class_num)\n",
        "for i in range(class_num):\n",
        "    rgba = cmap(i)\n",
        "    color_ls.append(colors.rgb2hex(rgba))"
      ],
      "id": "70062849-08d5-4e12-aaf7-be33046f4bdd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e53a1f46-fd55-47b6-960d-3373307951d3"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_dim=d):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(dataset.num_features, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x = F.relu(self.fc1(data.x))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim=d):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = pyg_nn.GCNConv(dataset.num_features, hidden_dim)\n",
        "        self.conv2 = pyg_nn.GCNConv(hidden_dim, hidden_dim)\n",
        "        self.fc1 = nn.Linear(hidden_dim, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class ChebNet(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim=d):\n",
        "        super(ChebNet, self).__init__()\n",
        "        self.conv1 = pyg_nn.ChebConv(dataset.num_features, hidden_dim, K=2)\n",
        "        self.conv2 = pyg_nn.ChebConv(hidden_dim, hidden_dim, K=2)\n",
        "        self.fc1 = nn.Linear(hidden_dim, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class ARMANet(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim=d):\n",
        "        super(ARMANet, self).__init__()\n",
        "        self.conv1 = pyg_nn.ARMAConv(dataset.num_features, hidden_dim)\n",
        "        self.conv2 = pyg_nn.ARMAConv(hidden_dim, hidden_dim)\n",
        "        self.fc1 = nn.Linear(hidden_dim, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, dim=d):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = pyg_nn.SAGEConv(dataset.num_features, dim)\n",
        "        self.conv2 = pyg_nn.SAGEConv(dim, dim*2, normalize=True)\n",
        "        self.fc1 = nn.Linear(dim*2, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class TAGCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim=d):\n",
        "        super(TAGCN, self).__init__()\n",
        "        self.conv1 = pyg_nn.TAGConv(dataset.num_features, hidden_dim)\n",
        "        self.conv2 = pyg_nn.TAGConv(hidden_dim, hidden_dim)\n",
        "        self.fc1 = nn.Linear(hidden_dim, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self, dim=d):\n",
        "        super(GIN, self).__init__()\n",
        "        nn1 = nn.Sequential(nn.Linear(dataset.num_features, dim*2), nn.ReLU(), nn.Linear(dim*2, dim))\n",
        "        nn2 = nn.Sequential(nn.Linear(dim, dim*2), nn.ReLU(), nn.Linear(dim*2, dim))\n",
        "        self.conv1 = pyg_nn.GINConv(nn1)\n",
        "        self.conv2 = pyg_nn.GINConv(nn2)\n",
        "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, dim=d):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = pyg_nn.GATConv(dataset.num_features, dim, edge_dim=edge_attr_all.shape[1])\n",
        "        self.conv2 = pyg_nn.GATConv(dim, dim, edge_dim=edge_attr_all.shape[1])\n",
        "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, edge_attr_all\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class MPNN(torch.nn.Module):\n",
        "    def __init__(self, dim=d):\n",
        "        super(MPNN, self).__init__()\n",
        "        nn1 = nn.Sequential(nn.Linear(edge_attr_all.shape[1], 16), nn.ReLU(), nn.Linear(16, dataset.num_features*dim))\n",
        "        self.conv1 = pyg_nn.NNConv(dataset.num_features, dim, nn1)\n",
        "        nn2 = nn.Sequential(nn.Linear(edge_attr_all.shape[1], 16), nn.ReLU(), nn.Linear(16, dim*dim))\n",
        "        self.conv2 = pyg_nn.NNConv(dim, dim, nn2)\n",
        "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, edge_attr_all #data.edge_attr\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class CGC(torch.nn.Module):\n",
        "    def __init__(self, dim=d):\n",
        "        super(CGC, self).__init__()\n",
        "        self.conv1 = pyg_nn.CGConv(dataset.num_features, edge_attr_all.size(-1))\n",
        "        self.conv2 = pyg_nn.CGConv(dataset.num_features, edge_attr_all.size(-1))\n",
        "        self.fc1 = nn.Linear(dataset.num_features, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, edge_attr_all\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_attr)) #\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "class GraphTransformer(torch.nn.Module):\n",
        "    def __init__(self, dim=d):\n",
        "        super(GraphTransformer, self).__init__()\n",
        "        self.conv1 = pyg_nn.TransformerConv(dataset.num_features, dim, edge_dim=edge_attr_all.shape[1])\n",
        "        self.conv2 = pyg_nn.TransformerConv(dim, dim, edge_dim=edge_attr_all.shape[1])\n",
        "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, edge_attr_all\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class GEN(torch.nn.Module):\n",
        "    def __init__(self, dim=d):\n",
        "        super(GEN, self).__init__()\n",
        "        self.node_encoder = nn.Linear(data.x.size(-1), dim)\n",
        "        self.edge_encoder = nn.Linear(edge_attr_all.size(-1), dim)\n",
        "        self.conv1 = pyg_nn.GENConv(dim, dim)\n",
        "        self.conv2 = pyg_nn.GENConv(dim, dim)\n",
        "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index, edge_attr = self.node_encoder(data.x), data.edge_index, self.edge_encoder(edge_attr_all)\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
        "        x = self.fc1(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "class TRAVELNet(torch.nn.Module):\n",
        "    def __init__(self, dim=d):\n",
        "        super(TRAVELNet, self).__init__()\n",
        "        convdim = 8\n",
        "        self.node_encoder = nn.Sequential(nn.Linear(data.x.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
        "        self.edge_encoder_dir = nn.Sequential(nn.Linear(data.component_dir.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
        "        self.edge_encoder_ang = nn.Sequential(nn.Linear(data.component_ang.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
        "        nn1 = nn.Sequential(nn.Linear(dim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, convdim))\n",
        "        self.conv1 = TRAVELConv(dim, convdim, nn1)\n",
        "        nn2 = nn.Sequential(nn.Linear(2*convdim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dataset.num_classes))\n",
        "        self.conv2 = TRAVELConv(2*convdim, dataset.num_classes, nn2)\n",
        "        self.bn1 = nn.BatchNorm1d(convdim*2)\n",
        "        nn1_2 = nn.Sequential(nn.Linear(dim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, convdim))\n",
        "        self.conv1_2 = TRAVELConv(dim, convdim, nn1_2)\n",
        "        nn2_2 = nn.Sequential(nn.Linear(2*convdim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dataset.num_classes))\n",
        "        self.conv2_2 = TRAVELConv(2*convdim, dataset.num_classes, nn2_2)\n",
        "        self.bn2 = nn.BatchNorm1d(dataset.num_classes*2)\n",
        "        self.fc = nn.Linear(dataset.num_classes*2, dataset.num_classes)\n",
        "\n",
        "    def forward(self):\n",
        "        x, edge_index = self.node_encoder(data.x), data.edge_index\n",
        "        edge_attr_dir, edge_attr_ang = self.edge_encoder_dir(data.component_dir), self.edge_encoder_ang(data.component_ang)\n",
        "        x1 = F.relu(self.conv1(x, edge_index, edge_attr_dir))\n",
        "        x2 = F.relu(self.conv1_2(x, edge_index, edge_attr_ang))\n",
        "        x = torch.cat((x1, x2), axis=1)\n",
        "        x = self.bn1(x)\n",
        "        x = F.dropout(x, p=p, training=self.training)\n",
        "        x1 = F.relu(self.conv2(x, edge_index, edge_attr_dir))\n",
        "        x2 = F.relu(self.conv2_2(x, edge_index, edge_attr_ang))\n",
        "        x = torch.cat((x1, x2), axis=1)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "id": "e53a1f46-fd55-47b6-960d-3373307951d3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c89d6f8-8c11-4c0c-8212-c44d08035da8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for e in [('Miami', 'Florida'),('Los Angeles', 'California'),('Orlando', 'Florida'),\n",
        "          ('Dallas', 'Texas'),('Houston', 'Texas'),('New York', 'New York')]:\n",
        "    city_name, state_abbrev = e[0].lower().replace(\" \", \"_\"), us_state_to_abbrev[e[1]].lower()\n",
        "    city_format = e[0]+' ('+us_state_to_abbrev[e[1]]+')'\n",
        "    if os.path.exists(file_path+city_name+'_'+state_abbrev+'/processed'):\n",
        "        shutil.rmtree(file_path+city_name+'_'+state_abbrev+'/processed')\n",
        "    dataset = TRAVELDataset(file_path, city_name+'_'+state_abbrev)\n",
        "    data = dataset[0]\n",
        "    class_num = dataset.num_classes\n",
        "    # print(f'Number of graphs: {len(dataset)}')\n",
        "    # print(f'Number of node features: {dataset.num_features}')\n",
        "    # print(f'Number of edge features: {dataset.num_edge_features}')\n",
        "    # print(f'Number of classes: {dataset.num_classes}')\n",
        "    # print(f'Number of nodes: {data.num_nodes}')\n",
        "    # print(f'Number of edges: {data.num_edges}')\n",
        "    # print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "    # print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
        "    # print(f'Contains self-loops: {data.has_self_loops()}')\n",
        "    # print(f'Is undirected: {data.is_undirected()}')\n",
        "\n",
        "    # 60%, 20% and 20% for training, validation and test\n",
        "    data.train_mask, data.val_mask, data.test_mask = train_test_split_stratify(dataset, train_ratio=0.6, val_ratio=0.2, class_num=class_num)\n",
        "    sc = MinMaxScaler()\n",
        "    data.x[data.train_mask] = torch.tensor(sc.fit_transform(data.x[data.train_mask]), dtype=torch.float)\n",
        "    data.x[data.val_mask] = torch.tensor(sc.transform(data.x[data.val_mask]), dtype=torch.float)\n",
        "    data.x[data.test_mask] = torch.tensor(sc.transform(data.x[data.test_mask]), dtype=torch.float)\n",
        "\n",
        "    edge_attr_all = MinMaxScaler().fit_transform(data.edge_attr.cpu())\n",
        "    edge_attr_all = torch.tensor(edge_attr_all).float().to(device)\n",
        "\n",
        "    coords = data.coords.numpy()\n",
        "    gdf_pred = pd.DataFrame({'x': coords[:, 0], 'y': coords[:, 1], 'label': data.y.numpy()})\n",
        "    zip_iterator = zip(gdf_pred.index, gdf_pred[['x', 'y']].values)\n",
        "    pos_dict = dict(zip_iterator)\n",
        "    draw_with_labels(gdf_pred, 'Ground Truth')\n",
        "\n",
        "    X_train, X_test, y_train, y_test = data.x[data.train_mask].cpu().numpy(), data.x[data.test_mask].cpu().numpy(), data.y[data.train_mask].cpu().numpy(), data.y[data.test_mask].cpu().numpy()\n",
        "    start_time = time.time()\n",
        "    xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    xgb_clf.fit(X_train, y_train)\n",
        "    y_pred = xgb_clf.predict(X_test)\n",
        "    test_acc, test_f1, test_map, test_auc = accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='binary'), average_precision_score(y_test, y_pred), roc_auc_score(y_test, y_pred)\n",
        "    print('f1_score {:.5f} | AUC {:.5f} | Test Accuracy {:.5f} | MAP {:.5f}'.format(test_f1, test_auc, test_acc, test_map))\n",
        "    res = (round(test_f1*100, 2), round(test_auc*100, 2), round(test_acc*100, 2), round(test_map*100, 2))\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('XGBoost',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    data = data.to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = MLP().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'MLP', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('MLP',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = GCN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'GCN', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('GCN',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = ChebNet().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'ChebNet', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('ChebNet',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = ARMANet().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'ARMANet', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('ARMANet',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = GraphSAGE().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'GraphSAGE', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('GraphSAGE',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = TAGCN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'TAGCN', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('TAGCN',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = GIN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'GIN', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('GIN',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = GAT().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.007, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'GAT', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('GAT',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = MPNN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'MPNN', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('MPNN',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = CGC().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.015, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'CGC', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('CGC',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = GraphTransformer().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'Transformer', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('Transformer',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = GEN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'GEN', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('GEN',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)\n",
        "\n",
        "    # Note that directional and angular edge features are precomputed in our datasets\n",
        "    component_dir = np.concatenate((data.edge_attr.cpu(), data.edge_attr_dir.cpu()), axis=1)\n",
        "    component_ang = np.concatenate((data.edge_attr.cpu(), data.edge_attr_ang.cpu()), axis=1)\n",
        "    component_dir = StandardScaler().fit_transform(component_dir)\n",
        "    component_ang = StandardScaler().fit_transform(component_ang)\n",
        "    data.component_dir = torch.tensor(component_dir).float().to(device)\n",
        "    data.component_ang = torch.tensor(component_ang).float().to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = TRAVELNet().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "    res = train_loop(model, data, optimizer, num_epochs, 'TRAVEL', city_name)\n",
        "    t = round(time.time() - start_time, 2)\n",
        "    all_res.append((city_format,) + ('TRAVEL',) + res + (t,))\n",
        "    print(\"Execution time: %.4f seconds\" % t)"
      ],
      "id": "7c89d6f8-8c11-4c0c-8212-c44d08035da8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66c6c538-e22f-4670-b051-b0ce24a50dbd"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(all_res, columns=['City', 'Method', 'F1', 'AUC', 'Acc', 'MAP', 'Time'])\n",
        "print('# datasets:', df.shape[0] // len(df.Method.unique()))\n",
        "df"
      ],
      "id": "66c6c538-e22f-4670-b051-b0ce24a50dbd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebHxRGm-d58S"
      },
      "outputs": [],
      "source": [],
      "id": "ebHxRGm-d58S"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}